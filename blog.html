<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Blog | Xinzhi Xue</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/blog.css">
  </head>
  <body>
    <header class="s-header">
      <div class="header-logo">
        <a href="index.html"><img src="images/education_w.svg" alt="Homepage" style="width: 50px; height: 56px;"></a>
      </div>
      <div class="header-content">
        <nav class="row header-nav-wrap">
          <ul class="header-nav">
            <li><a href="index.html#hero">Home</a></li>
            <li><a href="index.html#about">About</a></li>
            <li><a href="index.html#portfolio">Research</a></li>
            <li><a href="index.html#services">Publications</a></li>
            <li><a href="index.html#testimonials">Music</a></li>
            <li><a href="blog.html">Blog</a></li>
            <li><a href="mailto:xinzhialex.xue@gmail.com" title="Contact">Contact</a></li>
          </ul>
        </nav>
      </div>
    </header>

    <section class="s-blog target-section h-dark-bg" style="padding-top: 8rem; padding-bottom: 6rem;">
      <div class="row blog-entries">
        <div class="column large-8 tab-full">

          <!-- Blog Entry: Deep Learning Foundations -->
          <article class="blog-post">
            <h2 class="text-light" style="font-size: 2.4rem; margin-bottom: 1.5rem;">
              <a href="#">Deep Learning Foundations: Building from Scratch</a>
            </h2>
            <h5 class="h5 text-light" style="margin-top: 0; color: #bbb;">Nov 26, 2025</h5>

            <div class="blog-content">
              <p class="text-light" style="font-size: 1.6rem; line-height: 1.8;">
                Understanding deep learning requires more than just using high-level frameworks. This post chronicles
                my journey building neural networks from the ground up, implementing everything from basic classifiers
                to modern CNNs and RNNs, gaining insights that frameworks often abstract away.
              </p>

              <!-- Deep Learning Foundations Section -->
              <div class="blog-section">
                <h3>Deep Learning Foundations</h3>
              </div>

              <div class="blog-content">
                <ul>
                  <li>
                    <strong>Core ML Algorithms:</strong> Implemented fundamental algorithms from scratch including
                    K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Softmax classifiers, and 2-layer
                    neural networks. This hands-on approach revealed the mathematical foundations that modern
                    frameworks abstract away.
                  </li>
                  <li>
                    <strong>Vectorization & Performance:</strong> Applied vectorization techniques for performance
                    optimization, achieving <strong>5x+ speedups</strong> by replacing nested loops with NumPy matrix
                    operations. Learned how proper matrix operations are crucial for scalable deep learning.
                  </li>
                  <li>
                    <strong>Modular Neural Networks:</strong> Built modular neural network components including
                    forward/backward propagation, multiple optimizers (SGD, Momentum, Adam), and numerical gradient
                    verification to ensure correctness.
                  </li>
                </ul>
              </div>

              <!-- Advanced Deep Learning Section -->
              <div class="blog-section">
                <h3>Advanced Deep Learning</h3>
              </div>

              <div class="blog-content">
                <ul>
                  <li>
                    <strong>Regularization Techniques:</strong> Implemented modern regularization methods including
                    batch normalization, layer normalization, group normalization, and dropout. Understanding these
                    techniques from scratch revealed how they enable training deeper networks and prevent overfitting.
                  </li>
                  <li>
                    <strong>Convolutional Neural Networks:</strong> Built CNNs from scratch including convolution
                    layers, max pooling, and spatial normalization. Applied these concepts using PyTorch on the
                    CIFAR-10 dataset, bridging the gap between theory and practical implementation.
                  </li>
                  <li>
                    <strong>Recurrent Neural Networks:</strong> Developed RNNs for sequence modeling by implementing
                    vanilla RNN with backpropagation through time (BPTT). Applied the architecture to image captioning
                    on the COCO dataset, learning how to handle sequential data and temporal dependencies.
                  </li>
                </ul>
              </div>

              <!-- Key Takeaways -->
              <div class="key-features">
                <h4>Key Takeaways</h4>
                <ul>
                  <li><strong>Mathematics Matters:</strong> Understanding gradient descent, backpropagation, and chain rule is essential</li>
                  <li><strong>Vectorization is Critical:</strong> Matrix operations provide massive performance gains over loops</li>
                  <li><strong>Modular Design:</strong> Building reusable components makes complex architectures manageable</li>
                  <li><strong>Theory + Practice:</strong> Implementing from scratch, then using frameworks provides deepest understanding</li>
                  <li><strong>Debugging Skills:</strong> Numerical gradient checking is invaluable for validating implementations</li>
                </ul>
              </div>

              <div class="blog-note" style="margin-top: 3rem;">
                <strong>Resources:</strong> This learning was guided by Stanford's CS231 Deep Learning for Computer Vision
                and CS224 Natural Language Processing with Deep Learning courses. The combination of implementing from scratch
                and applying to real datasets like CIFAR-10 and COCO provided both theoretical depth and practical experience.
              </div>

            </div>
          </article>

          <!-- Blog Entry: LLM Offline -->
          <article class="blog-post" style="margin-top: 6rem; padding-top: 4rem; border-top: 2px solid #333;">
            <h2 class="text-light" style="font-size: 2.4rem; margin-bottom: 1.5rem;">
              <a href="#">Run LLM Completely Offline</a>
            </h2>
            <h5 class="h5 text-light" style="margin-top: 0; color: #bbb;">Feb 9, 2025</h5>

            <div class="blog-content">
              <p class="text-light" style="font-size: 1.6rem; line-height: 1.8;">
                Imagine you're solo backpacking in Yosemite — magnificent waterfalls, a sky glittering with stars.
                And suddenly, you feel the irresistible urge to chat with someone, not just anyone, a chatbot to be precise.
                This guide shows you how to summon one in total isolation, no internet required, so you're never truly off the grid.
              </p>

              <!-- Step 0 -->
              <div class="blog-section">
                <h3>Step 0: Activate Python Virtual Environment</h3>
              </div>
              <pre><code>python3 -m venv myenv
source myenv/bin/activate</code></pre>

              <!-- Step 1 -->
              <div class="blog-section">
                <h3>Step 1: Prepare Files on an Internet-Connected Machine</h3>
                <h4>1.1 Download Model & Dependencies</h4>
              </div>
              <pre><code># Install git-lfs first (on the connected machine)
git lfs install
git clone https://huggingface.co/deepseek-ai/deepseek-r1-distill-qwen-1.5b /path/to/deepseek-r1

# Create requirements.txt
echo "torch
transformers" > requirements.txt

# Download Python wheels for Apple Silicon (ARM64)
pip download -r requirements.txt --platform macosx_13_0_arm64 --only-binary=:all:</code></pre>

              <div class="blog-section">
                <h4>1.2 Build SentencePiece Wheel (Optional)</h4>
              </div>
              <p class="text-light" style="font-size: 1.5rem;">
                If you need to build the SentencePiece library from source for ARM64:
              </p>

              <pre><code># Install dependencies
xcode-select --install
brew install cmake

# Clone repository
git clone https://github.com/google/sentencepiece.git
cd sentencepiece

# Install Python build tools
pip install setuptools wheel</code></pre>

              <div class="blog-note">
                <strong>Note:</strong> Before building, in the <code>sentencepiece/python</code> folder:
                <ol style="margin-top: 1rem;">
                  <li>Replace <code>src</code> with <code>python/src</code> in all files</li>
                  <li>Replace <code>build_bundled.sh</code> with <code>python/build_bundled.sh</code></li>
                  <li>Replace <code>_version.py</code> with <code>python/_version.py</code></li>
                </ol>
              </div>

              <pre><code># Build the wheel
cd /path/to/sentencepiece
python python/setup.py bdist_wheel

# Find the .whl file in dist/ folder
# Install it
pip install dist/sentencepiece-*.whl</code></pre>

              <div class="blog-section">
                <h4>1.3 Package for Transfer</h4>
              </div>
              <pre><code>mkdir deepseek-offline
mv deepseek-r1 requirements.txt *.whl deepseek-offline/
zip -r deepseek-offline.zip deepseek-offline</code></pre>

              <!-- Step 2 -->
              <div class="blog-section">
                <h3>Step 2: Transfer to MacBook Pro</h3>
              </div>
              <ol>
                <li>Copy <code>deepseek-offline.zip</code> via USB/external drive</li>
                <li>Unzip on your Mac:</li>
              </ol>
              <pre><code>unzip deepseek-offline.zip -d ~/Documents/deepseek-offline</code></pre>

              <!-- Step 3 -->
              <div class="blog-section">
                <h3>Step 3: Install Docker for Apple Silicon</h3>
              </div>
              <ol>
                <li>Download <strong>Docker Desktop for Apple Silicon</strong> (ARM64) from <a href="https://docs.docker.com/desktop/install/mac-install/" style="color: #39b54a;">docker.com</a></li>
                <li>Install offline:
                  <ul style="margin-top: 0.5rem;">
                    <li>Double-click the <code>.dmg</code> file</li>
                    <li>Drag Docker to Applications folder</li>
                  </ul>
                </li>
                <li>Launch Docker Desktop and complete initial setup (no sign-in needed)</li>
              </ol>

              <!-- Step 4 -->
              <div class="blog-section">
                <h3>Step 4: Build the Offline Docker Image</h3>
                <h4>4.1 Create Dockerfile</h4>
              </div>
              <pre><code>cd ~/Documents/deepseek-offline
cat &lt;&lt;EOF &gt; Dockerfile
# Apple Silicon-compatible base image
FROM python:3.13-slim

# Install system dependencies
RUN echo "APT::Get::Assume-Yes \"true\";" &gt; /etc/apt/apt.conf.d/99force-yes && \
    echo 'APT::Get::AllowUnauthenticated "true";' &gt;&gt; /etc/apt/apt.conf.d/99force-yes && \
    apt-get update && apt-get install -y --no-install-recommends \
    libopenblas-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy offline packages
COPY *.whl /tmp/wheels/
COPY requirements.txt .

# Install dependencies offline
RUN pip install --find-links /tmp/wheels/ -r requirements.txt

# Copy model
COPY deepseek-r1-distill-qwen-1.5b /app/model

# Set cache environment variable to writable directory
RUN mkdir -p /tmp/huggingface_cache && chmod -R 777 /tmp/huggingface_cache
ENV HF_HOME=/tmp/huggingface_cache

# Prevent telemetry and network calls
ENV TRANSFORMERS_OFFLINE=1
ENV HF_DATASETS_OFFLINE=1

WORKDIR /app
EOF</code></pre>

              <div class="blog-section">
                <h4>4.2 Build the Image</h4>
              </div>
              <pre><code>docker build -t deepseek-r1-offline .</code></pre>

              <!-- Step 5 -->
              <div class="blog-section">
                <h3>Step 5: Run with Full Isolation</h3>
              </div>
              <pre><code># Run with NO network and read-only filesystem
docker run --rm \
  --network none \
  --read-only \
  --mount type=tmpfs,destination=/tmp \
  deepseek-r1-offline \
  python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained('/app/model', local_files_only=True).to('cpu')
tokenizer = AutoTokenizer.from_pretrained('/app/model', local_files_only=True)

inputs = tokenizer('Explain quantum physics simply:', return_tensors='pt')
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
"</code></pre>

              <!-- Step 6 -->
              <div class="blog-section">
                <h3>Step 6: Verify No Data Leakage</h3>
              </div>

              <p class="text-light" style="font-size: 1.5rem; margin-bottom: 1rem;">
                <strong>1. Confirm Docker container has no network:</strong>
              </p>
              <pre><code># While container is running (in another terminal)
docker exec -it &lt;CONTAINER_ID&gt; sh
ping 8.8.8.8  # Should fail immediately</code></pre>

              <p class="text-light" style="font-size: 1.5rem; margin-bottom: 1rem; margin-top: 2rem;">
                <strong>2. Monitor host machine traffic:</strong>
              </p>
              <pre><code>sudo lsof -i -P -n | grep ESTABLISHED  # Should show no Docker-related connections</code></pre>

              <!-- Optional: Persistent Script -->
              <div class="blog-section">
                <h3>Optional: Persistent Inference Script</h3>
                <h4>1. Create inference.py</h4>
              </div>
              <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import torch
import threading

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('/app/model', local_files_only=True)
model = AutoModelForCausalLM.from_pretrained('/app/model', local_files_only=True).to('cpu')

def run_inference(prompt, max_length=200):
    """Run inference with streaming output"""
    inputs = tokenizer(prompt, return_tensors="pt")

    # Create streamer for token-by-token output
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # Run generation in separate thread
    def generate():
        model.generate(
            inputs.input_ids,
            max_length=max_length,
            streamer=streamer
        )

    threading.Thread(target=generate).start()

    # Stream tokens as they're generated
    for token in streamer:
        print(token, end="", flush=True)

if __name__ == "__main__":
    run_inference("Explain quantum physics simply:")</code></pre>

              <div class="blog-section">
                <h4>2. Run through Docker</h4>
              </div>
              <pre><code>docker run --rm \
  --network none \
  --read-only \
  --mount type=tmpfs,destination=/tmp \
  -v ~/Documents/deepseek-offline/inference.py:/app/inference.py \
  deepseek-r1-offline \
  python inference.py</code></pre>

              <!-- Key Security Features -->
              <div class="key-features">
                <h4>Key Security Features</h4>
                <ul>
                  <li><code>--network none</code>: Kernel-level network isolation</li>
                  <li><code>--read-only</code>: Container cannot modify its filesystem</li>
                  <li><code>tmpfs mount</code>: Ephemeral storage wiped on exit</li>
                  <li><code>TRANSFORMERS_OFFLINE=1</code>: Disables Hugging Face telemetry</li>
                  <li>Apple Silicon-native build: Avoids x86 emulation vulnerabilities</li>
                  <li><code>local_files_only=True</code>: Prevents any network model downloads</li>
                </ul>
              </div>

              <!-- Cleanup -->
              <div class="blog-section">
                <h3>Cleanup</h3>
              </div>
              <pre><code># Remove container and image when done
docker rmi deepseek-r1-offline</code></pre>

              <div class="blog-note" style="margin-top: 3rem;">
                <strong>Performance Note:</strong> The use of MPS (<code>device='mps'</code>) can leverage your Mac's GPU
                for faster inference while keeping everything local. Replace <code>.to('cpu')</code> with <code>.to('mps')</code>
                in the code above if you want GPU acceleration.
              </div>

            </div>
          </article>
        </div>
      </div>
    </section>

    <footer class="s-footer h-dark-bg">
      <div class="row s-footer__bottom">
        <div class="column large-full ss-copyright">
          <span>© Xinzhi Xue 2020</span>
        </div>
        <div class="ss-go-top">
          <a class="smoothscroll" title="Back to Top" href="#top">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M12 0l8 9h-6v15h-4v-15h-6z"/></svg>
          </a>
        </div>
      </div>
    </footer>

    <script src="js/jquery-3.2.1.min.js"></script>
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>
  </body>
</html>
